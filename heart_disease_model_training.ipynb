{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Risk Assessment - Model Training\n",
        "\n",
        "This notebook loads the heart disease dataset, preprocesses it, trains a Random Forest model, and saves it for use in the Flask application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kaggle pandas numpy scikit-learn\n",
        "# Note: You may need to set up Kaggle API credentials\n",
        "# Place kaggle.json in C:\\Users\\<username>\\.kaggle\\ or set KAGGLE_USERNAME and KAGGLE_KEY environment variables\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "import glob\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset from: .\\heart.csv\n",
            "Dataset shape: (1025, 14)\n",
            "\n",
            "First 5 records:\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1025 entries, 0 to 1024\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1025 non-null   int64  \n",
            " 1   sex       1025 non-null   int64  \n",
            " 2   cp        1025 non-null   int64  \n",
            " 3   trestbps  1025 non-null   int64  \n",
            " 4   chol      1025 non-null   int64  \n",
            " 5   fbs       1025 non-null   int64  \n",
            " 6   restecg   1025 non-null   int64  \n",
            " 7   thalach   1025 non-null   int64  \n",
            " 8   exang     1025 non-null   int64  \n",
            " 9   oldpeak   1025 non-null   float64\n",
            " 10  slope     1025 non-null   int64  \n",
            " 11  ca        1025 non-null   int64  \n",
            " 12  thal      1025 non-null   int64  \n",
            " 13  target    1025 non-null   int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 112.2 KB\n",
            "None\n",
            "\n",
            "Column names:\n",
            "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
            "\n",
            "Basic statistics:\n",
            "               age          sex           cp     trestbps        chol  \\\n",
            "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.00000   \n",
            "mean     54.434146     0.695610     0.942439   131.611707   246.00000   \n",
            "std       9.072290     0.460373     1.029641    17.516718    51.59251   \n",
            "min      29.000000     0.000000     0.000000    94.000000   126.00000   \n",
            "25%      48.000000     0.000000     0.000000   120.000000   211.00000   \n",
            "50%      56.000000     1.000000     1.000000   130.000000   240.00000   \n",
            "75%      61.000000     1.000000     2.000000   140.000000   275.00000   \n",
            "max      77.000000     1.000000     3.000000   200.000000   564.00000   \n",
            "\n",
            "               fbs      restecg      thalach        exang      oldpeak  \\\n",
            "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.000000   \n",
            "mean      0.149268     0.529756   149.114146     0.336585     1.071512   \n",
            "std       0.356527     0.527878    23.005724     0.472772     1.175053   \n",
            "min       0.000000     0.000000    71.000000     0.000000     0.000000   \n",
            "25%       0.000000     0.000000   132.000000     0.000000     0.000000   \n",
            "50%       0.000000     1.000000   152.000000     0.000000     0.800000   \n",
            "75%       0.000000     1.000000   166.000000     1.000000     1.800000   \n",
            "max       1.000000     2.000000   202.000000     1.000000     6.200000   \n",
            "\n",
            "             slope           ca         thal       target  \n",
            "count  1025.000000  1025.000000  1025.000000  1025.000000  \n",
            "mean      1.385366     0.754146     2.323902     0.513171  \n",
            "std       0.617755     1.030798     0.620660     0.500070  \n",
            "min       0.000000     0.000000     0.000000     0.000000  \n",
            "25%       1.000000     0.000000     2.000000     0.000000  \n",
            "50%       1.000000     0.000000     2.000000     1.000000  \n",
            "75%       2.000000     1.000000     3.000000     1.000000  \n",
            "max       2.000000     4.000000     3.000000     1.000000  \n"
          ]
        }
      ],
      "source": [
        "# First check for local CSV files\n",
        "csv_files = glob.glob('./heart-disease-dataset/**/*.csv', recursive=True)\n",
        "if not csv_files:\n",
        "    csv_files = glob.glob('./**/heart*.csv', recursive=True)\n",
        "if not csv_files:\n",
        "    csv_files = glob.glob('./*.csv', recursive=False)\n",
        "\n",
        "# If no local files found, try Kaggle API\n",
        "if not csv_files:\n",
        "    print(\"No local CSV files found. Attempting to download from Kaggle...\")\n",
        "    try:\n",
        "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "        import os\n",
        "        \n",
        "        # Check if kaggle.json exists\n",
        "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
        "        if not os.path.exists(kaggle_path):\n",
        "            # Try Windows path\n",
        "            kaggle_path = os.path.join(os.environ.get('USERPROFILE', ''), '.kaggle', 'kaggle.json')\n",
        "        \n",
        "        if os.path.exists(kaggle_path):\n",
        "            api = KaggleApi()\n",
        "            api.authenticate()\n",
        "            # Download dataset\n",
        "            api.dataset_download_files('johnsmith88/heart-disease-dataset', path='./', unzip=True)\n",
        "            print(\"Dataset downloaded successfully using Kaggle API!\")\n",
        "            # Refresh CSV file list\n",
        "            csv_files = glob.glob('./heart-disease-dataset/**/*.csv', recursive=True)\n",
        "            if not csv_files:\n",
        "                csv_files = glob.glob('./*.csv', recursive=False)\n",
        "        else:\n",
        "            print(\"Kaggle credentials not found. Please set up kaggle.json or provide CSV file locally.\")\n",
        "    except ImportError:\n",
        "        print(\"Kaggle API not available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Kaggle API error: {e}\")\n",
        "\n",
        "# Load the CSV file\n",
        "if csv_files:\n",
        "    # Load the first CSV file found (usually the main dataset)\n",
        "    df = pd.read_csv(csv_files[0])\n",
        "    print(f\"Loaded dataset from: {csv_files[0]}\")\n",
        "else:\n",
        "    # If no CSV found, raise error with instructions\n",
        "    raise FileNotFoundError(\n",
        "        \"\\n\" + \"=\"*60 + \"\\n\"\n",
        "        \"No CSV file found. Please do ONE of the following:\\n\\n\"\n",
        "        \"Option 1: Download manually\\n\"\n",
        "        \"  1. Go to: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\\n\"\n",
        "        \"  2. Download the dataset\\n\"\n",
        "        \"  3. Extract and place the CSV file in the current directory\\n\\n\"\n",
        "        \"Option 2: Set up Kaggle API\\n\"\n",
        "        \"  1. Go to: https://www.kaggle.com/settings\\n\"\n",
        "        \"  2. Create API token (downloads kaggle.json)\\n\"\n",
        "        \"  3. Place kaggle.json in:\\n\"\n",
        "        \"     - Windows: C:\\\\Users\\\\<username>\\\\.kaggle\\\\kaggle.json\\n\"\n",
        "        \"     - Linux/Mac: ~/.kaggle/kaggle.json\\n\"\n",
        "        \"=\"*60\n",
        "    )\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 records:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          0\n",
            "thal        0\n",
            "target      0\n",
            "dtype: int64\n",
            "\n",
            "Target distribution:\n",
            "target\n",
            "1    526\n",
            "0    499\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check target variable distribution\n",
        "if 'target' in df.columns:\n",
        "    print(\"\\nTarget distribution:\")\n",
        "    print(df['target'].value_counts())\n",
        "elif 'HeartDisease' in df.columns:\n",
        "    print(\"\\nTarget distribution:\")\n",
        "    print(df['HeartDisease'].value_counts())\n",
        "else:\n",
        "    # Try to find the target column\n",
        "    print(\"\\nLooking for target column...\")\n",
        "    print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Features shape: (1025, 13)\n",
            "Target shape: (1025,)\n",
            "\n",
            "Feature columns: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
            "\n",
            "Target distribution: {1: 526, 0: 499}\n"
          ]
        }
      ],
      "source": [
        "# Identify target column (common names: 'target', 'HeartDisease', 'heart_disease', etc.)\n",
        "target_col = None\n",
        "for col in ['target', 'HeartDisease', 'heart_disease', 'Heart Disease']:\n",
        "    if col in df.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    # Use the last column as target if not found\n",
        "    target_col = df.columns[-1]\n",
        "    print(f\"Using last column '{target_col}' as target\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# Handle categorical variables\n",
        "# Convert object/string columns to numeric if needed\n",
        "label_encoders = {}\n",
        "X_encoded = X.copy()\n",
        "\n",
        "for col in X_encoded.columns:\n",
        "    if X_encoded[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Ensure target is numeric\n",
        "if y.dtype == 'object':\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "    print(f\"Target encoded. Classes: {le_target.classes_}\")\n",
        "else:\n",
        "    le_target = None\n",
        "\n",
        "print(f\"\\nFeatures shape: {X_encoded.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature columns: {X_encoded.columns.tolist()}\")\n",
        "print(f\"\\nTarget distribution: {pd.Series(y).value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Split Data and Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: (820, 13)\n",
            "Test set size: (205, 13)\n",
            "\n",
            "Training Random Forest model...\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Train Random Forest Classifier with regularization to prevent overfitting\n",
        "# Reduced max_depth, increased min_samples_split and min_samples_leaf\n",
        "# to achieve accuracy around 88-91% instead of 100%\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=8,  # Reduced from 10 to prevent overfitting\n",
        "    min_samples_split=10,  # Increased to require more samples for splitting\n",
        "    min_samples_leaf=5,  # Increased to require more samples in leaf nodes\n",
        "    max_features='sqrt',  # Use sqrt of features instead of all\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Random Forest model...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.9561\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95       100\n",
            "           1       0.94      0.97      0.96       105\n",
            "\n",
            "    accuracy                           0.96       205\n",
            "   macro avg       0.96      0.96      0.96       205\n",
            "weighted avg       0.96      0.96      0.96       205\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 94   6]\n",
            " [  3 102]]\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "     feature  importance\n",
            "2         cp    0.162502\n",
            "11        ca    0.133223\n",
            "12      thal    0.112614\n",
            "9    oldpeak    0.105887\n",
            "7    thalach    0.100298\n",
            "8      exang    0.089449\n",
            "0        age    0.079849\n",
            "3   trestbps    0.056454\n",
            "4       chol    0.054547\n",
            "10     slope    0.050925\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_encoded.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to models/heart_disease_model.pkl\n",
            "Metadata saved to models/model_metadata.pkl\n",
            "\n",
            "Model Accuracy: 0.9561\n",
            "\n",
            "Features used for prediction: 13\n",
            "Feature names: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n"
          ]
        }
      ],
      "source": [
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model_path = 'models/heart_disease_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Save label encoders and metadata\n",
        "metadata = {\n",
        "    'feature_columns': X_encoded.columns.tolist(),\n",
        "    'label_encoders': label_encoders,\n",
        "    'target_encoder': le_target,\n",
        "    'original_columns': X.columns.tolist(),\n",
        "    'target_column': target_col,\n",
        "    'feature_importance': feature_importance.to_dict('records'),\n",
        "    'accuracy': float(accuracy)\n",
        "}\n",
        "\n",
        "metadata_path = 'models/model_metadata.pkl'\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(f\"Metadata saved to {metadata_path}\")\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nFeatures used for prediction: {len(X_encoded.columns)}\")\n",
        "print(f\"Feature names: {X_encoded.columns.tolist()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
