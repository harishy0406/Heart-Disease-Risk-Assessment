{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Risk Assessment - Model Training\n",
        "\n",
        "This notebook loads the heart disease dataset, preprocesses it, trains a Random Forest model, and saves it for use in the Flask application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the latest version of the dataset\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"johnsmith88/heart-disease-dataset\",\n",
        "    \"\",  # Empty string to load all files or specify file path\n",
        ")\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 records:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check target variable distribution\n",
        "if 'target' in df.columns:\n",
        "    print(\"\\nTarget distribution:\")\n",
        "    print(df['target'].value_counts())\n",
        "elif 'HeartDisease' in df.columns:\n",
        "    print(\"\\nTarget distribution:\")\n",
        "    print(df['HeartDisease'].value_counts())\n",
        "else:\n",
        "    # Try to find the target column\n",
        "    print(\"\\nLooking for target column...\")\n",
        "    print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify target column (common names: 'target', 'HeartDisease', 'heart_disease', etc.)\n",
        "target_col = None\n",
        "for col in ['target', 'HeartDisease', 'heart_disease', 'Heart Disease']:\n",
        "    if col in df.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    # Use the last column as target if not found\n",
        "    target_col = df.columns[-1]\n",
        "    print(f\"Using last column '{target_col}' as target\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# Handle categorical variables\n",
        "# Convert object/string columns to numeric if needed\n",
        "label_encoders = {}\n",
        "X_encoded = X.copy()\n",
        "\n",
        "for col in X_encoded.columns:\n",
        "    if X_encoded[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Ensure target is numeric\n",
        "if y.dtype == 'object':\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "    print(f\"Target encoded. Classes: {le_target.classes_}\")\n",
        "else:\n",
        "    le_target = None\n",
        "\n",
        "print(f\"\\nFeatures shape: {X_encoded.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature columns: {X_encoded.columns.tolist()}\")\n",
        "print(f\"\\nTarget distribution: {pd.Series(y).value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Split Data and Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Random Forest model...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_encoded.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model_path = 'models/heart_disease_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Save label encoders and metadata\n",
        "metadata = {\n",
        "    'feature_columns': X_encoded.columns.tolist(),\n",
        "    'label_encoders': label_encoders,\n",
        "    'target_encoder': le_target,\n",
        "    'original_columns': X.columns.tolist(),\n",
        "    'target_column': target_col,\n",
        "    'feature_importance': feature_importance.to_dict('records'),\n",
        "    'accuracy': float(accuracy)\n",
        "}\n",
        "\n",
        "metadata_path = 'models/model_metadata.pkl'\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(f\"Metadata saved to {metadata_path}\")\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nFeatures used for prediction: {len(X_encoded.columns)}\")\n",
        "print(f\"Feature names: {X_encoded.columns.tolist()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
